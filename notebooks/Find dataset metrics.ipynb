{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50b02a4-afe3-4b9a-b6d7-862e313d05c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d93b8e70-1b13-4b94-8150-4c9f8b02de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data import load_data\n",
    "from multilabel.loader import MultiLabelDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b975bc31-b4f3-4e5f-9396-c95faec6f4ba",
   "metadata": {
    "id": "0w24yf-Tj47H"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from numpy.random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.autograd.profiler as tprofiler\n",
    "import torch.utils.data as td\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deeddf6b-ec3d-46f7-8f52-b3e4d1f81d4c",
   "metadata": {
    "id": "O7W8BTtF3BN1"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "\n",
    "# pytorch RNGs\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# numpy RNG\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab4f91ac-bf77-4376-b36e-1a51d38616f8",
   "metadata": {
    "id": "21_bts2Wj47M",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"../data/processed\"\n",
    "images_dir = \"../data/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5252376-80da-41de-8a9e-6238b5f97cf0",
   "metadata": {
    "id": "Tw4oZtz0j47N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(images_dir,\"train.csv\"))\n",
    "val = pd.read_csv(os.path.join(images_dir,\"val.csv\"))\n",
    "test = pd.read_csv(os.path.join(images_dir,\"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "486f40f3-3dfa-4e06-8942-454c70a8eff2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "GrhqYNEPj47N",
    "outputId": "ae41146c-5e99-4e5f-dcae-8166dd2c2bbc",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Active</th>\n",
       "      <th>Alert</th>\n",
       "      <th>Amazed</th>\n",
       "      <th>Amused</th>\n",
       "      <th>Calm</th>\n",
       "      <th>Cheerful</th>\n",
       "      <th>Confident</th>\n",
       "      <th>Conscious</th>\n",
       "      <th>Creative</th>\n",
       "      <th>...</th>\n",
       "      <th>Educated</th>\n",
       "      <th>Emotional</th>\n",
       "      <th>Fashionable</th>\n",
       "      <th>Feminine</th>\n",
       "      <th>Inspired</th>\n",
       "      <th>Loving</th>\n",
       "      <th>Manly</th>\n",
       "      <th>Persuaded</th>\n",
       "      <th>Thrifty</th>\n",
       "      <th>Youthful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6040.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>159230.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Path  Active  Alert  Amazed  Amused  Calm  Cheerful  Confident  \\\n",
       "0    6040.jpg     NaN    2.0     NaN     NaN   NaN       NaN        NaN   \n",
       "1  159230.jpg     NaN    NaN     NaN     NaN   NaN       NaN        NaN   \n",
       "\n",
       "   Conscious  Creative  ...  Educated  Emotional  Fashionable  Feminine  \\\n",
       "0        NaN       NaN  ...       NaN        NaN          NaN       NaN   \n",
       "1        NaN       NaN  ...       2.0        NaN          NaN       NaN   \n",
       "\n",
       "   Inspired  Loving  Manly  Persuaded  Thrifty  Youthful  \n",
       "0       NaN     NaN    NaN        NaN      NaN       NaN  \n",
       "1       NaN     NaN    NaN        NaN      NaN       NaN  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80ac86ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11135it [00:29, 380.63it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in tqdm(train.iterrows()):\n",
    "    img = Image.open(os.path.join(data_dir, \"images\", \"train\", row.Path))\n",
    "    img = np.array(img)\n",
    "    # find number of channels\n",
    "    if img.ndim != 3:\n",
    "        print(f\"{row.Path} - image has {img.ndim} channel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d41b8eeb-90a1-453d-a7c2-23e54922d9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3712it [00:10, 367.97it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in tqdm(test.iterrows()):\n",
    "    img = Image.open(os.path.join(data_dir, \"images\", \"test\", row.Path))\n",
    "    img = np.array(img)\n",
    "    # find number of channels\n",
    "    if img.ndim != 3:\n",
    "        print(f\"{row.Path} - image has {img.ndim} channel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "324f1b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3712it [00:09, 386.14it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in tqdm(val.iterrows()):\n",
    "    img = Image.open(os.path.join(data_dir, \"images\", \"val\", row.Path))\n",
    "    img = np.array(img)\n",
    "    # find number of channels\n",
    "    if img.ndim != 3:\n",
    "        print(f\"{row.Path} - image has {img.ndim} channel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccc7759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ec7a71c-ba12-468e-8d73-1b2ee9116c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 64\n",
    "image_size = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ff5601b-4b32-4c04-914a-16a463ecf5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = T.Compose([T.Resize(size=(image_size, image_size)),\n",
    "                 T.ToTensor(),\n",
    "                 T.Normalize(mean = (0, 0, 0),\n",
    "                              std  = (1, 1, 1))])\n",
    "\n",
    "\n",
    "train = pd.read_csv(os.path.join(images_dir, \"train.csv\")).set_index(\"Path\").fillna(0)\n",
    "test = pd.read_csv(os.path.join(images_dir, \"test.csv\")).set_index(\"Path\").fillna(0)\n",
    "val = pd.read_csv(os.path.join(images_dir, \"val.csv\")).set_index(\"Path\").fillna(0)\n",
    "\n",
    "\n",
    "train_dataset = MultiLabelDataset(root=os.path.join(data_dir, \"images\", \"train\"),\n",
    "                                     dataframe=train,\n",
    "                                     transform=aug)\n",
    "val_dataset = MultiLabelDataset(root=os.path.join(data_dir, \"images\", \"val\"),\n",
    "                                   dataframe=val,\n",
    "                                   transform=aug)\n",
    "test_dataset = MultiLabelDataset(root=os.path.join(data_dir, \"images\", \"test\"),\n",
    "                                    dataframe=test,\n",
    "                                    transform=aug)\n",
    "\n",
    "data_loader_train = td.DataLoader(train_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=False,\n",
    "                                  drop_last=False,\n",
    "                                  num_workers=num_workers,\n",
    "                                  pin_memory=True)\n",
    "data_loader_val = td.DataLoader(val_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                drop_last=False,\n",
    "                                num_workers=num_workers,\n",
    "                                pin_memory=True)\n",
    "data_loader_test = td.DataLoader(test_dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=False,\n",
    "                                 drop_last=False,\n",
    "                                 num_workers=num_workers,\n",
    "                                 pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6a41e8d-adba-4505-b52b-5bb44ab8ac69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [01:05<00:00,  2.65it/s]\n"
     ]
    }
   ],
   "source": [
    "psum    = torch.tensor([0.0, 0.0, 0.0])\n",
    "psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "\n",
    "# loop through images\n",
    "for inputs, labels in tqdm(data_loader_train):\n",
    "    psum    += inputs.sum(axis        = [0, 2, 3])\n",
    "    psum_sq += (inputs ** 2).sum(axis = [0, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d5c5785-1d3a-4dc2-8286-06fd146764e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor([0.5989, 0.5510, 0.5175])\n",
      "std:  tensor([0.3358, 0.3330, 0.3377])\n"
     ]
    }
   ],
   "source": [
    "count = len(train) * image_size * image_size\n",
    "\n",
    "# mean and std\n",
    "total_mean = psum / count\n",
    "total_var  = (psum_sq / count) - (total_mean ** 2)\n",
    "total_std  = torch.sqrt(total_var)\n",
    "\n",
    "# output\n",
    "print('mean: '  + str(total_mean))\n",
    "print('std:  '  + str(total_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3b127-fee8-4225-8451-eb33e087186e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8df119e9-08b9-4b90-9a1c-842d8fbd0ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:22<00:00,  2.63it/s]\n"
     ]
    }
   ],
   "source": [
    "psum    = torch.tensor([0.0, 0.0, 0.0])\n",
    "psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "\n",
    "# loop through images\n",
    "for inputs, labels in tqdm(data_loader_val):\n",
    "    psum    += inputs.sum(axis        = [0, 2, 3])\n",
    "    psum_sq += (inputs ** 2).sum(axis = [0, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b1fb5ac-34ef-4864-94a5-ad8b3187dbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor([0.5945, 0.5479, 0.5135])\n",
      "std:  tensor([0.3362, 0.3323, 0.3360])\n"
     ]
    }
   ],
   "source": [
    "count = len(val) * image_size * image_size\n",
    "\n",
    "# mean and std\n",
    "total_mean = psum / count\n",
    "total_var  = (psum_sq / count) - (total_mean ** 2)\n",
    "total_std  = torch.sqrt(total_var)\n",
    "\n",
    "# output\n",
    "print('mean: '  + str(total_mean))\n",
    "print('std:  '  + str(total_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e3458-23f7-4472-92d3-f3ba348de9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8efa995-00f6-4d29-b02e-a99832cb978b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:21<00:00,  2.68it/s]\n"
     ]
    }
   ],
   "source": [
    "psum    = torch.tensor([0.0, 0.0, 0.0])\n",
    "psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "\n",
    "# loop through images\n",
    "for inputs, labels in tqdm(data_loader_test):\n",
    "    psum    += inputs.sum(axis        = [0, 2, 3])\n",
    "    psum_sq += (inputs ** 2).sum(axis = [0, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d93c5238-fd26-44e9-8d6f-8caf7620ea93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor([0.6009, 0.5539, 0.5198])\n",
      "std:  tensor([0.3367, 0.3324, 0.3383])\n"
     ]
    }
   ],
   "source": [
    "count = len(test) * image_size * image_size\n",
    "\n",
    "# mean and std\n",
    "total_mean = psum / count\n",
    "total_var  = (psum_sq / count) - (total_mean ** 2)\n",
    "total_std  = torch.sqrt(total_var)\n",
    "\n",
    "# output\n",
    "print('mean: '  + str(total_mean))\n",
    "print('std:  '  + str(total_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ffe414-f01b-4917-92f4-1672f78add44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae8f8ffa-e8a7-482b-ab08-0eb3796c20c8",
   "metadata": {},
   "source": [
    "## Calculating pos_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42a8da61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3323., 1489.,  144.,  452.,  233.,  569.,  722.,  437., 5109.,\n",
       "       6792., 1100.,  137., 5941., 3567., 1342.,  126.,  806.,  450.,\n",
       "        382.,  363.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sum().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9bc88c0c-9a80-46d1-939a-0f895b1fed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pos_weights(class_counts, data):\n",
    "    pos_weights = np.ones_like(class_counts)\n",
    "    neg_counts = [len(data)-pos_count for pos_count in class_counts]\n",
    "    for cdx, (pos_count, neg_count) in enumerate(zip(class_counts,  neg_counts)):\n",
    "        pos_weights[cdx] = neg_count / (pos_count + 1e-5)\n",
    "\n",
    "    return torch.as_tensor(pos_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d964b4d-0198-48d8-be2b-8dd911b9ea96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.3509,  6.4782, 76.3264, 23.6350, 46.7897, 18.5694, 14.4224, 24.4805,\n",
       "         1.1795,  0.6394,  9.1227, 80.2774,  0.8743,  2.1217,  7.2973, 87.3730,\n",
       "        12.8151, 23.7444, 28.1492, 29.6749])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_pos_weights([3323., 1489.,  144.,  452.,  233.,  569.,  722.,  437., 5109.,\n",
    "       6792., 1100.,  137., 5941., 3567., 1342.,  126.,  806.,  450.,\n",
    "        382.,  363.], data_loader_train.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d9f2247a-c9c8-47cf-9580-e55adb98e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9edcd06e-bb97-44a0-b222-37e33adcb97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = None\n",
    "b = None\n",
    "for inp, tar in data_loader_train:\n",
    "    a = inp\n",
    "    b = tar\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa0afa5-4b85-4f5f-89c9-9fa87f5f8685",
   "metadata": {},
   "outputs": [],
   "source": [
    "[2., 3., 6., 6., 3., 7., 9.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b7e9babc-06cf-4b85-947f-6556261df62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931)\n",
      "tensor(0.8912)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "loss = criterion(torch.zeros(64, 7), b)\n",
    "print(loss)\n",
    "\n",
    "criterion_weighted = nn.BCEWithLogitsLoss(pos_weight=torch.as_tensor([2., 3., 6., 6., 3., 7., 9.], dtype=torch.float))\n",
    "loss_weighted = criterion_weighted(torch.zeros(64, 7), b)\n",
    "print(loss_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c2c27bc-e921-4854-ab9b-46232aadb6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931)\n",
      "tensor(1.1569)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "loss = criterion(torch.zeros(64, 7), b)\n",
    "print(loss)\n",
    "\n",
    "criterion_weighted = nn.BCEWithLogitsLoss(pos_weight=torch.as_tensor(20450*7/25186, dtype=torch.float))\n",
    "loss_weighted = criterion_weighted(torch.zeros(64, 7), b)\n",
    "print(loss_weighted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8663be1127523d7121742bbf948a8b1c8dd9a63c15e224e5108ff87b090569d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
